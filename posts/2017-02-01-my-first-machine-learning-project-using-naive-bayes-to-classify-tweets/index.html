<!DOCTYPE html>
<html lang="en"><head>

  <meta name="generator" content="Hugo 0.71.1" />
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="author" content="Pang Yan Han"><meta name="keywords" content="machine learning"><meta property="og:title" content="My first Machine Learning project: Using Naive Bayes to classify tweets" />
<meta property="og:description" content="Source code: https://gist.github.com/yanhan/d9061c9575d14228d2a9ecc9519a55aa
Before we go into the main content of this post, I have a confession to make - it&rsquo;s pretty damned hard to get started with Machine Learning if you&rsquo;re not doing it on the job and if you don&rsquo;t happen to have some problem to solve (and have the appropriate data for it). One of the main hurdles I&rsquo;ve faced is that after picking up some fundamentals, Machine Learning knowledge is like this hammer but there aren&rsquo;t many suitable nails in sight, probably because of my lack of experience; a lot of problems also seem very, very tough." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://yanhan.github.io/posts/2017-02-01-my-first-machine-learning-project-using-naive-bayes-to-classify-tweets/" />
<meta property="article:published_time" content="2017-02-01T21:34:00+00:00" />
<meta property="article:modified_time" content="2017-02-01T21:34:00+00:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="My first Machine Learning project: Using Naive Bayes to classify tweets"/>
<meta name="twitter:description" content="Source code: https://gist.github.com/yanhan/d9061c9575d14228d2a9ecc9519a55aa
Before we go into the main content of this post, I have a confession to make - it&rsquo;s pretty damned hard to get started with Machine Learning if you&rsquo;re not doing it on the job and if you don&rsquo;t happen to have some problem to solve (and have the appropriate data for it). One of the main hurdles I&rsquo;ve faced is that after picking up some fundamentals, Machine Learning knowledge is like this hammer but there aren&rsquo;t many suitable nails in sight, probably because of my lack of experience; a lot of problems also seem very, very tough."/>

  <link rel="stylesheet" type="text/css" media="screen" href="https://yanhan.github.io/css/normalize.css" />
  <link rel="stylesheet" type="text/css" media="screen" href="https://yanhan.github.io/css/main.css" />
  <link rel="stylesheet" type="text/css" media="screen" href="https://yanhan.github.io/css/all.css" />
<link rel="stylesheet" type="text/css" media="screen" href="https://yanhan.github.io/css/custom.css" /><title>My first Machine Learning project: Using Naive Bayes to classify tweets | Yan Han&#39;s blog</title><script type="text/javascript">(function(c,a){if(!a.__SV){var b=window;try{var d,m,j,k=b.location,f=k.hash;d=function(a,b){return(m=a.match(RegExp(b+"=([^&]*)")))?m[1]:null};f&&d(f,"state")&&(j=JSON.parse(decodeURIComponent(d(f,"state"))),"mpeditor"===j.action&&(b.sessionStorage.setItem("_mpcehash",f),history.replaceState(j.desiredHash||"",c.title,k.pathname+k.search)))}catch(n){}var l,h;window.mixpanel=a;a._i=[];a.init=function(b,d,g){function c(b,i){var a=i.split(".");2==a.length&&(b=b[a[0]],i=a[1]);b[i]=function(){b.push([i].concat(Array.prototype.slice.call(arguments,0)))}}var e=a;"undefined"!==typeof g?e=a[g]=[]:g="mixpanel";e.people=e.people||[];e.toString=function(b){var a="mixpanel";"mixpanel"!==g&&(a+="."+g);b||(a+=" (stub)");return a};e.people.toString=function(){return e.toString(1)+".people (stub)"};l="disable time_event track track_pageview track_links track_forms track_with_groups add_group set_group remove_group register register_once alias unregister identify name_tag set_config reset opt_in_tracking opt_out_tracking has_opted_in_tracking has_opted_out_tracking clear_opt_in_out_tracking people.set people.set_once people.unset people.increment people.append people.union people.track_charge people.clear_charges people.delete_user people.remove".split(" ");for(h=0;h<l.length;h++)c(e,l[h]);var f="set set_once union unset remove delete".split(" ");e.get_group=function(){function a(c){b[c]=function(){call2_args=arguments;call2=[c].concat(Array.prototype.slice.call(call2_args,0));e.push([d,call2])}}for(var b={},d=["get_group"].concat(Array.prototype.slice.call(arguments,0)),c=0;c<f.length;c++)a(f[c]);return b};a._i.push([b,d,g])};a.__SV=1.2;b=c.createElement("script");b.type="text/javascript";b.async=!0;b.src="undefined"!==typeof MIXPANEL_CUSTOM_LIB_URL? MIXPANEL_CUSTOM_LIB_URL:"file:"===c.location.protocol&&"//cdn4.mxpnl.com/libs/mixpanel-2-latest.min.js".match(/^\/\//)?"https://cdn4.mxpnl.com/libs/mixpanel-2-latest.min.js":"//cdn4.mxpnl.com/libs/mixpanel-2-latest.min.js";d=c.getElementsByTagName("script")[0];d.parentNode.insertBefore(b,d)}})(document,window.mixpanel||[]);mixpanel.init("49f4d9dd919ecb7ee428489acdd19dff");</script>
</head>
<body><header>

  <div id="titletext"><h2 id="title"><a href="https://yanhan.github.io/">Yan Han&#39;s blog</a></h2></div>
  <div id="title-description"><p id="subtitle">On Computer Technology</p><div id=social>
    <nav>
      <ul><li><a href="https://github.com/yanhan"><i title="GitHub" class="icons fab fa-github"></i></a></li><li><a href="/index.xml"><i title="RSS" class="icons fas fa-rss"></i></a></li></ul>
    </nav>
  </div>
  </div>
  <div id="mainmenu">
    <nav>
      <ul>
        
        <li><a href="/">Home</a></li>
        
        <li><a href="/about">About</a></li>
        
        <li><a href="/bookshelf">Bookshelf</a></li>
        
        <li><a href="/posts">All Posts</a></li>
        
        <li><a href="/tags">Tags</a></li>
        
      </ul>
    </nav>
  </div>
</header>
<main><div class="post">
<div class="author">

</div>
<div class="post-header">

<div class="meta">

<div class="date">
<span class="day">01</span>
<span class="rest">Feb 2017</span>
</div>

</div>

<div class="matter">
<h1 class="title">My first Machine Learning project: Using Naive Bayes to classify tweets</h1>
</div>
</div>

<div class="tags">









<table>
  <tbody>
    <tr>
      <td>
        <p>Tags</p>
      </td>
      <td class="tagvalues">
        <p>
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        <a href="/tags/machine-learning/"> machine-learning </a>
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
      </td>
    </tr>
  </tbody>
</table>
</div>





</div>

<div class="markdown">
<p>Source code: <a href="https://gist.github.com/yanhan/d9061c9575d14228d2a9ecc9519a55aa">https://gist.github.com/yanhan/d9061c9575d14228d2a9ecc9519a55aa</a></p>
<p>Before we go into the main content of this post, I have a confession to make - it&rsquo;s pretty damned hard to get started with Machine Learning if you&rsquo;re not doing it on the job and if you don&rsquo;t happen to have some problem to solve (and have the appropriate data for it). One of the main hurdles I&rsquo;ve faced is that after picking up some fundamentals, Machine Learning knowledge is like this hammer but there aren&rsquo;t many suitable nails in sight, probably because of my lack of experience; a lot of problems also seem very, very tough. This post is my latest attempt to build up my skills in it - through some hands on experience on &ldquo;real-world&rdquo; data sets. Hopefully this trend keeps up and I&rsquo;ll have many more projects under my belt over the course of the year.</p>
<h2 id="inspiration">Inspiration</h2>
<p>Recently, I&rsquo;ve finished reading John Foreman&rsquo;s <a href="http://a.co/9BR5zhg">Data Smart</a> and got quite inspired to try out some techniques in there. I spent an entire afternoon brainstorming about some project ideas. One of them is, why not do a variant of what&rsquo;s covered in Data Smart chapter 3, which is, to use Naive Bayes to classify tweets? Except that, we&rsquo;ll be using a programming language instead of Excel (thank goodness).</p>
<h2 id="the-problem">The problem</h2>
<p>Alright, we&rsquo;ve decided on the algorithm - Naive Bayes. To make things simple, we will build a binary class classifier, just like in the book. So, what kind of tweets? The best kind of tweets are probably those involving some word with ambiguous meaning. This word may or may not be used as a hashtag in the tweets. The word &lsquo;react&rsquo; was one of the first which came to my mind - after all, it is used in conventional speech / writing and is the name of a popular JavaScript framework, so the same word stands for two very distinct things/meanings. A human reading the tweets should not have much issue labelling the data but more importantly, these 2 attributes of the word &lsquo;react&rsquo; should garner a sufficiently large number of tweets for both classes.</p>
<h2 id="obtaining-the-data">Obtaining the data</h2>
<p>I wish I could say that there was a data set that I just downloaded from somewhere or I used the Twitter API to do this, but I did a search on Twitter and manually copy and pasted tweets into 2 files, one for tweets about the React framework and the other containing tweets that are not about the React framework. For each class, I collected 250 tweets and all / most of them contain the word &lsquo;react&rsquo; or &lsquo;React&rsquo;. This was a very painful and laborious process that took a few hours to complete. In particular, most tweets about the React JS framework do not contain the word &lsquo;react&rsquo; or &lsquo;React&rsquo; but contain &lsquo;ReactJS&rsquo; (maybe in a different case), so it took a long time for me to find suitable tweets.</p>
<p>For each class, I shuffled the 250 tweets and partitioned them into a training set of 180 tweets (72% of the data) and a test set of 70 tweets (28% of the data).</p>
<h2 id="a-tiny-bit-of-exploratory-data-analysis-and-thoughts-before-starting">A tiny bit of exploratory data analysis and thoughts before starting</h2>
<p>This is a toy problem and it is extremely easy to classify tweets about React JS correctly - in fact we don&rsquo;t need machine learning to do it. During the process of gathering the data, other than the fact that most tweets about React JS contain the word <code>ReactJS</code> in some kind of case variant or use the hashtag <code>#ReactJS</code>, they typically don&rsquo;t use the word <code>react</code> and even if they do, it is almost always spelt <code>React</code> with an uppercase R instead of <code>react</code>. Whereas in tweets not about the React JS framework, we don&rsquo;t see the uppercase <code>React</code> - it is always <code>react</code>. Ok, to make things more fun, we lowercase everything.</p>
<p>On the <code>#ReactJS</code> hashtag - by summing the token counts, we see that other than the token <code>react</code>, the next most common occurring token in the training set is <code>#reactjs</code> at 115 counts, followed by <code>#javascript</code> at 61 counts. Again, to make things more fun, we discard these two tokens.</p>
<h2 id="the-hand-rolled-classifiers">The hand-rolled classifiers</h2>
<p>There are 2 implementations: one hand-coded Naive Bayes implementation and one using scikit-learn.</p>
<p>For the hand-coded implementation, we follow most of the implementation in Data Smart. For each tweet, we first replace all occurrences of <code>.</code>, <code>:</code>, <code>?</code>, <code>!</code>, <code>;</code>, <code>,</code> which are followed by a space character with a single space character. Then we split on whitespace and reject all tokens with three characters or less, along with the tokens <code>#reactjs</code> and <code>#javascript</code>. We train 2 classifiers - one for recognizing tweets about React JS and the other for recognizing tweets not about React JS. Instead of the additive smoothing method covered in the book (which adds 1 to the count for every token), we use the <a href="Additive_smoothing">Additive smoothing</a> covered on Wikipedia and <a href="http://stats.stackexchange.com/q/108797">this Cross Validated question</a>:</p>
<p>$$P(token | class) = \frac{x_i + \alpha}{N + \alpha * |V|} $$</p>
<p>where \(x_i\) is the token count in the class, \(\alpha = 1\), \( N \) is the sum of all token counts in the given class, and \( |V| \) is the size of the vocabulary in the entire training set (regardless of class). This is identical to the formula in the Cross Validated question, except that it doesn&rsquo;t contain an additional \( + 1 \) in the denominator.</p>
<p>To handle tokens which are not present in the training set but present in the test set, we default to using this probability:</p>
<p>$$P(unseen\ token | class) = \frac{1}{N + |V|}$$</p>
<p>When passed a tweet, each classifier computes the sum of the log likelihood of each token in the tweet. We pass the tweet to both classifiers and compare the two log probabilities - the higher one wins and we say that the tweet belongs to that class.</p>
<p>The confusion matrix is as follows:</p>
<p><img src="/images/2017-02-01/hand-rolled-confusion-matrix.png" alt=""></p>
<p>Looks pretty good.</p>
<h2 id="the-scikit-learn-classifier">The scikit-learn classifier</h2>
<p>Originally, I didn&rsquo;t want to do this because this is just a toy project but, since I&rsquo;m doing it already, might as well figure out how to implement Naive Bayes using scikit-learn on something simple like this.</p>
<p>After reading some stuff from the following links in the awesome scikit-learn documentation:</p>
<ul>
<li><a href="http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html">http://scikit-learn.org/stable/modules/naive_bayes.html</a></li>
<li><a href="http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html">http://scikit-learn.org/stable/auto_examples/applications/plot_out_of_core_classification.html</a></li>
<li><a href="http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html">http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html</a></li>
</ul>
<p>I figured out how to implement something similar to our hand-rolled Naive Bayes classifier using scikit-learn. Instead of using <code>CountVectorizer</code>, I decided to use <code>HashingVectorizer</code>. The hardest part in this implementation is to figure out how to use the <code>HashingVectorizer</code>. Eventually, it came down to this:</p>
<pre><code class="language-{lang="python"}" data-lang="{lang="python"}">stop_words = list(
    set(
        sklearn.feature_extraction.text.ENGLISH_STOP_WORDS
    ).union(_BANNED_TOKENS)
)
vectorizer = HashingVectorizer(
    stop_words=stop_words,
    token_pattern=r&quot;&quot;&quot;\b\w\w\w\w+\b&quot;&quot;&quot;,
    norm=None,
    non_negative=True,
)
</code></pre><p>We needed to add the stopwords <code>#reactjs</code> and <code>#javascript</code> to the default list of stopwords that the <code>HashingVectorizer</code> was using. A little googling yielded this useful <a href="http://stackoverflow.com/a/24386751">answer</a>. By default, <code>HashingVectorizer</code> uses the <code>u'(?u)\b\w\w+\b'</code> regex which captures any token of length 2 or more, but we only want tokens which are at least length 4, so we used <code>token_pattern=r&quot;&quot;&quot;\b\w\w\w\w+\b&quot;&quot;&quot;</code> to override that setting. We also didn&rsquo;t want any normalization (default is <code>'l2'</code>) and we didn&rsquo;t want any non-negative values - just raw counts.</p>
<p>We used the <code>HashingVectorizer.transform</code> method to transform all the training tweets into a sparse matrix, and fed that to <code>sklearn.naive_bayes.MultinomialNB</code> via its <code>fit</code> method. To evaluate this model on our test data, we need to use the <code>HashingVectorizer</code> to transform the test data, then pass them to the trained Naive Bayes model.</p>
<p>The confusion matrix for the scikit-learn Multinomial Naive Bayes model:</p>
<p><img src="/images/2017-02-01/sklearn-naive-bayes-confusion-matrix.png" alt=""></p>
<p>While this classifier performs better than our hand-rolled classifier for tweets about ReactJS, it performs worse for tweets that are not about ReactJS.</p>
<h2 id="analysis">Analysis</h2>
<h3 id="some-statistics-of-our-models">Some statistics of our models</h3>
<p><img src="/images/2017-02-01/hand-rolled-log-prob-unseen-token.png" alt=""></p>
<p>The number 1895 refers to the size of the vocabulary for the entire training set (both tweets about React JS and not about React JS). The log probability for an unseen token is higher for the React JS model as compared to the non React JS model. The pitfall is that a tweet which consists entirely of unseen tokens will favor the React JS model - most likely that tweet is not about React JS, since most tweets about React JS will have some variant of the <code>#reactjs</code> hashtag which will exist in the training set.</p>
<p>For the MultinomialNB model, the numbers are -13.86442357 for a non React JS tweet and -13.86489007 for a React JS tweet. A much smaller discrepancy that slightly favors the non React JS class.</p>
<h3 id="false-negatives-for-our-hand-rolled-naive-bayes-model">False negatives for our hand rolled Naive Bayes model:</h3>
<ol>
<li>Why I like Vue over React - <a href="http://buff.ly/2jWKpzJ">http://buff.ly/2jWKpzJ</a>  #vuejs #reactjs</li>
<li>The power of React JS&hellip;  <a href="http://catchoftheday.wesbos.com/store/lazy-glamorous-knives">http://catchoftheday.wesbos.com/store/lazy-glamorous-knives</a> …</li>
<li>these &ldquo;google trends&rdquo; charts can show almost whatever you want. people search for &ldquo;React&rdquo; rather than &ldquo;React.js&rdquo; <a href="http://image.prntscr.com/image/42ff33c9b0da42ef8084f45a647ccc00.png">http://image.prntscr.com/image/42ff33c9b0da42ef8084f45a647ccc00.png</a> …</li>
</ol>
<p>For the first tweet, the only tokens that are counted are <code>like</code>, <code>over</code>, <code>react</code>, <code>http://buff.ly/2jWKpzJ</code> and <code>#vuejs</code>. Among those, only <code>#vuejs</code> has predictive power, since Vue.js is a JavaScript framework. However, the token <code>#vuejs</code> does not occur in the training set. So it is understandable that this tweet is misclassified.</p>
<p>It is a similar story for the second tweet, with only <code>JS...</code> being the token with predictive power. However, that doesn&rsquo;t occur in the training set.</p>
<p>For the final tweet, the only token with predictive power is <code>&quot;react.js&quot;</code> (notice the quotes), but the presence of the quotes probably screwed things up.</p>
<h3 id="false-negatives-for-the-sklearn-multinomialnb-model">False negatives for the sklearn MultinomialNB model:</h3>
<ol>
<li>these &ldquo;google trends&rdquo; charts can show almost whatever you want. people search for &ldquo;React&rdquo; rather than &ldquo;React.js&rdquo; <a href="http://image.prntscr.com/image/42ff33c9b0da42ef8084f45a647ccc00.png">http://image.prntscr.com/image/42ff33c9b0da42ef8084f45a647ccc00.png</a> …</li>
</ol>
<p>Which is pretty curious, since the vectorizer correctly tokenizes <code>&quot;React.js&quot;</code> into the token <code>react.js</code>.</p>
<h3 id="false-positives-for-hand-rolled-naive-bayes-model">False positives for hand rolled Naive Bayes model:</h3>
<ol>
<li>i may not react but trust me i saw it</li>
<li>Star Wars superfans (and one feisty Rebel soldier) react to #RogueOne. <a href="http://strw.rs/60138rHgt">http://strw.rs/60138rHgt</a></li>
<li>Ants react to their infection by climbing up plants and sinking their mandibles into plant tissue</li>
<li>@Khlil10x react to the NFL games</li>
<li>Why ppl wud insult my father? Ppl wud react if I wud use my fathers name an absolute truth 2 impose n violate others basic rights.</li>
</ol>
<h3 id="false-positives-for-sklearn-multinomialnb-model">False positives for sklearn MultinomialNB model:</h3>
<ol>
<li>How everyone should react</li>
<li>i may not react but trust me i saw it</li>
<li>Star Wars superfans (and one feisty Rebel soldier) react to #RogueOne. <a href="http://strw.rs/60138rHgt">http://strw.rs/60138rHgt</a></li>
<li>At this point, no one is believing 20% mexico surcharge is real. We&rsquo;d have to abrogate NAFTA in total to do that. That why no market react.</li>
<li>This is amazing. Jack Eichel&rsquo;s Dad &amp; other Sabres parents in Nashville react to Eichel&rsquo;s game-winning goal in OT.</li>
<li>Things to Consider: read the article &amp; not just the headline before you react &amp; retweet. Less exciting, but better for all.</li>
<li>Ants react to their infection by climbing up plants and sinking their mandibles into plant tissue</li>
<li>If you&rsquo;re a #veteran, how do you react when someone calls you a hero? Here&rsquo;s my perspective on it <a href="http://ow.ly/EKQE308ex4i">http://ow.ly/EKQE308ex4i</a></li>
<li>@Khlil10x react to the NFL games</li>
<li>Why ppl wud insult my father? Ppl wud react if I wud use my fathers name an absolute truth 2 impose n violate others basic rights.</li>
</ol>
<p>Notice that all the false positives for the Naive Bayes model are also false positives for the MultinomialNB model. If there&rsquo;s one thing I can say straight off the bat about these false positives, it is this - they are about pretty different things - there is a tweet about star wars, a tweet about ants, etc. This is a stark contrast to the tweets about ReactJS, which are well, about ReactJS. But all these is just guesswork - let&rsquo;s look at more concrete stuff.</p>
<h3 id="analysis-of-false-negatives-for-hand-rolled-naive-bayes-model">Analysis of false negatives for hand rolled Naive Bayes model</h3>
<p>For the tweet <code>Why I like Vue over React - http://buff.ly/2jWKpzJ  #vuejs #reactjs</code></p>
<p><img src="/images/2017-02-01/first-false-negative-analysis.png" alt=""></p>
<p>The main culprits are the tokens <code>over</code> and <code>like</code>. <code>over</code> is a stop word for scikit learn and its removal would have helped things slightly, but <code>like</code> is not a stop word and its relatively high log probability (compared to the other tokens which do not appear in the vocabulary) means that this tweet will still be incorrectly classified. Note that this tweet is not a false negative for the MultinomialNB model - some painstaking investigation into the log probabilities of the features of the MultinomialNB model revealed that the <code>reactjs</code> token greatly favored the React JS model and the <code>react</code> and <code>http</code> tokens slightly favored the React JS model; the other log probabilities are about even except for the token <code>like</code> favoring the non React JS model. Log probabilities given by the MultinomialNB model as follows:</p>
<p><img src="/images/2017-02-01/first-false-negative-multinomialnb.png" alt=""></p>
<p>For the tweet <code>The power of React JS...  http://catchoftheday.wesbos.com/store/lazy-glamorous-knives …</code></p>
<p><img src="/images/2017-02-01/second-false-negative-analysis.png" alt=""></p>
<p>The culprit is the token <code>power</code>, which appears twice in the training set for non reactjs tweets but does not appear in the training set for reactjs tweets. Note that this tweet is correctly classified by the MultinomialNB model - most of the log probabilities for the tokens are pretty even and the difference makers for the React JS model are the tokens <code>http</code>, <code>wesbos</code> (there was an occurrence of <code>@wesbos</code> in the training set and after the processing by <code>HashingVectorizer</code> it became <code>wesbos</code>) and <code>store</code> (appeared once in training set) - notice that these are all tokens created as a result of removing all the punctuation in the URL and treating them just like whitespace. Seems like in a bag of words model, there may be more value chopping up URLs on top of leaving them as they are or maybe even as opposed to leaving them as they are. Log probabilities given by the MultinomialNB as follows:</p>
<p><img src="/images/2017-02-01/second-false-negative-multinomialnb.png" alt=""></p>
<p>For the tweet <code>these &quot;google trends&quot; charts can show almost whatever you want. people search for &quot;React&quot; rather than &quot;React.js&quot; http://image.prntscr.com/image/42ff33c9b0da42ef8084f45a647ccc00.png …</code></p>
<p><img src="/images/2017-02-01/third-false-negative-analysis.png" alt=""></p>
<p>This is a rather long tweet so I am only showing some of the more interesting tokens here. First, the errors we made from tokenization - <code>trends&quot;</code>, <code>&quot;react&quot;</code>, <code>&quot;react.js&quot;</code>, <code>&quot;google</code>; these are highlighted in pink. In a larger data set, these might have affected results more because they will be unified with the correct tokens. In particular, the token <code>react.js</code> will favor the React JS model.</p>
<p>There are 5 stop words here, namely: rather, show, whatever, these, than. These would have been removed by the a scikit learn model using the default English stop words. Their presence in classification favors the non React JS model, which contains more of these tokens with the exception of <code>these</code>.</p>
<p>Finally, the tokens <code>people</code> and <code>want</code>, which are neither stop words nor incorrect tokens. Especially <code>people</code> - it greatly favors the non React JS model.</p>
<p>This tweet is also incorrectly classified by the MultinomialNB model. The HashingVectorizer will produce the following tokens: <code>google</code>, <code>trends</code>, <code>charts</code>, <code>want</code>, <code>people</code>, <code>search</code>, <code>react</code>, <code>http</code>, <code>image</code>, <code>prntscr</code>, <code>42ff33c9b0da42ef8084f45a647ccc00</code>. Of these, <code>react</code>, <code>http</code> and <code>image</code> give a slight edge to the React JS model, but it is not sufficient to counter the effect of the tokens <code>want</code> and especially <code>people</code>. The table below shows the log probabilities of these tokens given by the MultinomialNB model:</p>
<p><img src="/images/2017-02-01/third-false-negative-multinomialnb.png" alt=""></p>
<h3 id="analysis-of-false-positives-for-hand-rolled-naive-bayes-model">Analysis of false positives for hand rolled Naive Bayes model</h3>
<ol>
<li>i may not react but trust me i saw it</li>
<li>Star Wars superfans (and one feisty Rebel soldier) react to #RogueOne. <a href="http://strw.rs/60138rHgt">http://strw.rs/60138rHgt</a></li>
<li>Ants react to their infection by climbing up plants and sinking their mandibles into plant tissue</li>
<li>@Khlil10x react to the NFL games</li>
<li>Why ppl wud insult my father? Ppl wud react if I wud use my fathers name an absolute truth 2 impose n violate others basic rights.</li>
</ol>
<p>Tweet: <code>i may not react but trust me i saw it</code></p>
<p>Our hand rolled model sees the tokens <code>trust</code> and <code>react</code>, both of which slightly favor the React JS model - it is a very similar story for the MultinomialNB model. It seems that very short tweets like this one and tweets which contain many stop words (only 2 out of 10 tokens here are meaningful) can be easily misclassified by a Naive Bayes model.</p>
<p>Tweet: <code>Star Wars superfans (and one feisty Rebel soldier) react to #RogueOne. http://strw.rs/60138rHgt</code></p>
<p>More interesting log probabilities for hand rolled Naive Bayes model:</p>
<p><img src="/images/2017-02-01/second-false-positive-hand-roll.png" alt=""></p>
<p>More interesting log probabilities for the MultinomialNB model:</p>
<p><img src="/images/2017-02-01/second-false-positive-multinomialnb.png" alt=""></p>
<p>Once again we see the flaw of a bag of words model - just because a token appears more frequently for documents of a given class doesn&rsquo;t mean that a new document containing that token belongs to that class.</p>
<p>Tweet: <code>Ants react to their infection by climbing up plants and sinking their mandibles into plant tissue</code></p>
<p>More interesting log probabilities for our hand rolled model:</p>
<p><img src="/images/2017-02-01/third-false-positive-hand-roll.png" alt=""></p>
<p>Other than the above tokens, all the other tokens were unseen by both models. Recall that the log probability for an unseen token is about -8.1645 for the React JS model and -8.25088 for the non React JS model - these numbers add up to favor the React JS model, even if we remove all the stop words.</p>
<p>For the MultinomialNB model, it is the token <code>react</code> that made the big difference. All the other tokens are unseen in the training set, which gives a slight edge to the non React JS class but that was not enough to counter the effect of the <code>react</code> token.</p>
<p>Tweet: <code>@Khlil10x react to the NFL games</code></p>
<p>Another very short tweet.</p>
<p>Our hand rolled models sees the tokens <code>@khlil10x</code>, <code>react</code> and <code>games</code>. The only seen token is <code>react</code>, which favors the React JS model. Since log probs for unseen tokens are higher for the React JS model, everything favors the React JS model.</p>
<p>For the MultinomialNB model, it is again the token <code>react</code> that favored the React JS model. The other tokens <code>Khlil10x</code> and <code>games</code> are unseen and give a very very slight edge to the React JS model.</p>
<p>Tweet: <code>Why ppl wud insult my father? Ppl wud react if I wud use my fathers name an absolute truth 2 impose n violate others basic rights.</code></p>
<p>More interesting log probabilities for hand rolled model:</p>
<p><img src="/images/2017-02-01/fifth-false-positive-hand-roll.png" alt=""></p>
<p>Even though <code>insult</code> and <code>others</code> gave an edge to the non React JS model, it wasn&rsquo;t sufficient to offset the effects of the <code>basic</code> and <code>react</code> tokens along with 9 other unseen tokens.</p>
<p>More interesting log probabilities for MultinomialNB model:</p>
<p><img src="/images/2017-02-01/fifth-false-positive-multinomialnb.png" alt=""></p>
<p>It is a similar situation for the MultinomialNB model with <code>react</code> and <code>basic</code> tilting the scales towards the React JS class.</p>
<h3 id="false-positives-for-the-multinomialnb-model-that-are-not-false-positives-for-the-hand-rolled-naive-bayes-model">False positives for the MultinomialNB model that are not false positives for the hand rolled Naive Bayes model</h3>
<p>The bigger question is, why does the MultinomialNB model perform worse than our hand rolled Naive Bayes model? The following tweets were correctly classified as negatives by our hand rolled Naive Bayes model but incorrectly classified by the MultinomialNB model:</p>
<ol>
<li>How everyone should react</li>
<li>At this point, no one is believing 20% mexico surcharge is real. We&rsquo;d have to abrogate NAFTA in total to do that. That why no market react.</li>
<li>This is amazing. Jack Eichel&rsquo;s Dad &amp; other Sabres parents in Nashville react to Eichel&rsquo;s game-winning goal in OT.</li>
<li>Things to Consider: read the article &amp; not just the headline before you react &amp; retweet. Less exciting, but better for all.</li>
<li>If you&rsquo;re a #veteran, how do you react when someone calls you a hero? Here&rsquo;s my perspective on it <a href="http://ow.ly/EKQE308ex4i">http://ow.ly/EKQE308ex4i</a></li>
</ol>
<p>Tweet: <code>How everyone should react</code></p>
<p>After HashingVectorizer processes this tweet, the only remaining token is <code>react</code>, which favors the React JS class. <code>How</code>, <code>everyone</code> and <code>should</code> are stop words.</p>
<p>Stop words that will be considered by our hand rolled model: <code>everyone</code>, <code>should</code></p>
<p>Log probabilities of the stop words for hand rolled classifiers:</p>
<p><img src="/images/2017-02-01/multinomialnb-only-1st-false-positive-hand-rolled.png" alt=""></p>
<p>So it was indeed the log probabilities of stop words that influenced the outcome for our hand rolled classifiers.</p>
<p>Tweet: <code>At this point, no one is believing 20% mexico surcharge is real. We'd have to abrogate NAFTA in total to do that. That why no market react.</code></p>
<p>More interesting log probabilities:</p>
<p><img src="/images/2017-02-01/multinomialnb-only-2nd-false-positive.png" alt=""></p>
<p>Stop words that will be considered by our hand rolled model: <code>this</code>, <code>have</code>, <code>that</code></p>
<p>Log probabilities of stop words for hand rolled classifiers:</p>
<p><img src="/images/2017-02-01/multinomialnb-only-2nd-false-positive-hand-rolled.png" alt=""></p>
<p><code>that</code> appeared twice in this tweet and hence its log probability was counted twice. Notice the <code>react.</code> ending with a fullstop. Stop words definitely influenced the outcome for our hand rolled models.</p>
<p>Tweet: <code>This is amazing. Jack Eichel's Dad &amp; other Sabres parents in Nashville react to Eichel's game-winning goal in OT.</code></p>
<p>More interesting log probabilities:</p>
<p><img src="/images/2017-02-01/multinomialnb-only-3rd-false-positive.png" alt=""></p>
<p>Stop words that will be considered by our hand rolled model: <code>this</code>, <code>other</code></p>
<p>Log probabilities of stop words for hand rolled classifiers:</p>
<p><img src="/images/2017-02-01/multinomialnb-only-3rd-false-positive-hand-rolled.png" alt=""></p>
<p>Again, stop words do influence the outcome in this case.</p>
<p>Tweet: <code>Things to Consider: read the article &amp; not just the headline before you react &amp; retweet. Less exciting, but better for all.</code></p>
<p>More interesting log probabilities:</p>
<p><img src="/images/2017-02-01/multinomialnb-only-4th-false-positive.png" alt=""></p>
<p>Stop words that will be considered by our hand rolled model: <code>before</code>, <code>less</code></p>
<p>Log probabilities of stop words for hand rolled classifiers:</p>
<p><img src="/images/2017-02-01/multinomialnb-only-4th-false-positive-hand-rolled.png" alt=""></p>
<p>Tweet: <code>If you're a #veteran, how do you react when someone calls you a hero? Here's my perspective on it http://ow.ly/EKQE308ex4i</code></p>
<p>More interesting log probabilities:</p>
<p><img src="/images/2017-02-01/multinomialnb-only-5th-false-positive.png" alt=""></p>
<p>Stop words that will be considered by our hand rolled model: <code>when</code>, <code>someone</code></p>
<p>Log probabilities of stop words for hand rolled classifiers:</p>
<p><img src="/images/2017-02-01/multinomialnb-only-5th-false-positive-hand-rolled.png" alt=""></p>
<p>This is a more interesting case. While the stop word <code>when</code> tilts the scales towards the non React JS class, the token <code>you're</code> does so as well - this token does not exist in the vocabulary of the MultinomialNB classifier (it would have been chopped up into <code>you</code> and <code>re</code> and both discarded). Our hand rolled classifier also does not chop up the URL and so <code>http</code> isn&rsquo;t a token for our hand rolled models.</p>
<p>From these 5 examples, we see that stop word removal by the HashingVectorizer introduced more false positives.</p>
<h2 id="conclusion">Conclusion</h2>
<p>The Naive Bayes model works pretty well for a small data set and simple classification task like this one. For classification tasks involving text, it is probably a good starting point due to the simplicity of its implementation. As such, it makes a good baseline from which we can use to evaluate more sophisticated methods.</p>
<p>It is also pretty instructive to open up the model and figure out the reasons for misclassifications.</p>
<p>We summarize our findings here:</p>
<ol>
<li>short tweets are prone to misclassification as it is very likely for them to mostly contain stop words. The remaining tokens will get &ldquo;more weightage&rdquo;</li>
<li>tweets containing a lot of unseen tokens are also prone to misclassification for the same reason as 1</li>
<li>a bag of words model only takes into account of word frequencies and can be tricked. Context and other language structures will help in classification but they are totally not taken into account of by a bag of words model</li>
<li>stop words affect classification, as seen from our analysis of the false negatives for the MultinomialNB model.</li>
</ol>
<h2 id="afterthoughts">Afterthoughts</h2>
<p>Even though this is a toy problem, we learnt how to do the following:</p>
<ol>
<li>Hand roll Naive Bayes and at least have this one method to deal with text data</li>
<li>Use scikit-learn&rsquo;s <code>HashVectorizer</code> with some customizations</li>
<li>Inspect the Naive Bayes model to figure out the reasons for misclassifications</li>
</ol>
<p>And we got our hands dirty with machine learning outside of a course like environment or following instructions in a book / tutorial.</p>
<p>This also happens to be our first blog post in quite a while. To those reading this, Happy New Year folks!</p>

</div>

<div class="disclaimer">
  <p>Disclaimer: Opinions expressed on this blog are solely my own and do not express the views or opinions of my employer(s), past or present.</p>
</div><div id="disqus_thread"></div>
<script type="text/javascript">

(function () {
  
  
  if (window.location.hostname == "localhost")
    return;

  var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
  var disqus_shortname = 'pangyanhan';
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();
</script>
<noscript>Please enable JavaScript to view the</a></noscript>
<a href="http://disqus.com/">comments powered by <span>Disqus</span></a>
</div>

<script>mixpanel.track("page view", {"page": "https:\/\/yanhan.github.io\/posts\/2017-02-01-my-first-machine-learning-project-using-naive-bayes-to-classify-tweets\/"})</script>

</main><footer>
 © Copyright 2013 Yan Han Pang | <a href="https://github.com/dataCobra/hugo-vitae">Vitae</a> theme for <a href="https://gohugo.io">Hugo</a> 


    

</footer>
</body>
</html>
